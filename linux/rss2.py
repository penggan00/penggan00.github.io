import asyncio
import aiohttp
import logging
import re
import os
import hashlib
import pytz
import fcntl
import sqlite3
import time
import signal
import sys
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv
from feedparser import parse
from telegram import Bot
from telegram.error import BadRequest
from tencentcloud.common import credential
from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from tencentcloud.tmt.v20180321 import tmt_client, models
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from md2tgmd import escape
from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# é…ç½®ç»å¯¹è·¯å¾„
BASE_DIR = Path(__file__).resolve().parent
# åˆ›å»ºé”æ–‡ä»¶
LOCK_FILE = BASE_DIR / "rss.lock"
# SQLite æ•°æ®åº“åˆå§‹åŒ–
DATABASE_FILE = BASE_DIR / "rss.db"

# å¢å¼ºæ—¥å¿—é…ç½®
logging.basicConfig(
    filename=BASE_DIR / "rss.log",
    level=logging.WARNING,  # åªè®°å½• WARNING/ERROR/CRITICAL
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

logger = logging.getLogger(__name__)

TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID").split(",")
TENCENTCLOUD_SECRET_ID = os.getenv("TENCENTCLOUD_SECRET_ID")
TENCENTCLOUD_SECRET_KEY = os.getenv("TENCENTCLOUD_SECRET_KEY")
TENCENT_REGION = os.getenv("TENCENT_REGION", "na-siliconvalley")
# åœ¨ç¯å¢ƒå˜é‡åŠ è½½åæ·»åŠ å¤‡ç”¨å¯†é’¥é…ç½®
TENCENT_SECRET_ID = os.getenv("TENCENT_SECRET_ID")
TENCENT_SECRET_KEY = os.getenv("TENCENT_SECRET_KEY")
semaphore = asyncio.Semaphore(2)  # å¹¶å‘æ§åˆ¶ï¼Œé™åˆ¶åŒæ—¶æœ€å¤š2ä¸ªè¯·æ±‚
# é…ç½®å¤‡ç”¨åŸŸåï¼ˆæœ€å¤šæ”¯æŒä»»æ„æ•°é‡ï¼‰
BACKUP_DOMAINS_STR = os.getenv("BACKUP_DOMAINS", "")
BACKUP_DOMAINS = [domain.strip() for domain in BACKUP_DOMAINS_STR.split(",") if domain.strip()]
# å®šä¹‰æ—¶é—´é—´éš” (ç§’)  600ç§’ = 10åˆ†é’Ÿ   1200ç§’ = 20åˆ†é’Ÿ   1800ç§’ = 30åˆ†é’Ÿ  3600ç§’ = 1å°æ—¶   7200ç§’ = 2å°æ—¶   10800ç§’ = 3å°æ—¶

RSS_GROUPS = [
    # ================== å›½é™…æ–°é—»ç»„ ==================False: å…³é—­ / True: å¼€å¯
    {
        "name": "å›½é™…æ–°é—»",
        "urls": [
            'https://feeds.bbci.co.uk/news/world/rss.xml',  # BBC
            'https://www3.nhk.or.jp/rss/news/cat6.xml',     # NHK
       #     'https://www.cnbc.com/id/100003114/device/rss/rss.html',  # CNBC
         #   'https://feeds.a.dj.com/rss/RSSWorldNews.xml',  # åå°”è¡—æ—¥æŠ¥
        #    'https://feeds.content.dowjones.io/public/rss/RSSWorldNews',   # åå°”è¡—æ—¥æŠ¥
        #    'https://feeds.content.dowjones.io/public/rss/socialeconomyfeed',
           'https://www.aljazeera.com/xml/rss/all.xml',    # åŠå²›ç”µè§†å°
        #    'https://www.ft.com/?format=rss',                 # é‡‘èæ—¶æŠ¥
       #     'https://www3.nhk.or.jp/rss/news/cat5.xml',  # NHK å•†ä¸š
       #     'http://rss.cnn.com/rss/cnn_topstories.rss',   # cnn
       #     'https://www.theguardian.com/world/rss',     # å«æŠ¥
      #      'https://www.theverge.com/rss/index.xml',   # The Verge:
        ],
        "group_key": "RSS_FEEDS",
        "interval": 3590,      # 55åˆ†é’Ÿ 
        "history_days": 30,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_TWO"),    # Telegram Bot Token
        "processor": {
            "translate": True,       #ç¿»è¯‘å¼€
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": False,         # ç¦æ­¢é¢„è§ˆ
            "show_count": False        # âœ…æ–°å¢
        }
    },

    # ================== å›½é™…æ–°é—»ä¸­æ–‡ç»„ ==================False: å…³é—­ / True: å¼€å¯
    {
        "name": "å›½é™…æ–°é—»ä¸­æ–‡",
        "urls": [
            'https://www.ftchinese.com/rss/news',   # ftä¸­æ–‡ç½‘
        ],
        "group_key": "RSS_FEEDS_INTERNATIONAL",
        "interval": 10790,      # 3å°æ—¶
        "history_days": 30,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_TWO"),    # Telegram Bot Token
        "processor": {
            "translate": False,       #ç¿»è¯‘å…³
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": False,         # ç¦æ­¢é¢„è§ˆ
            "show_count": False        # âœ…æ–°å¢
        }
    },

    # ================== å¿«è®¯ç»„ ==================
    {
        "name": "å¿«è®¯",
        "urls": [
    #        'https://rsshub.app/10jqka/realtimenews',
            'https://36kr.com/feed-newsflash',  # 36æ°ªå¿«è®¯
        #    'https://36kr.com/feed',  # 36æ°ªç»¼åˆ
            
        ],
        "group_key": "FOURTH_RSS_FEEDS",
        "interval": 960,       # 11åˆ†é’Ÿ 
        "history_days": 5,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_LINDA"),  
        "processor": {
            "translate": False,     #ç¿»è¯‘å¼€å…³
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": False,            # é¢„è§ˆ
            "show_count": False          #è®¡æ•°
        }
    },

    # ================== ç¤¾äº¤åª’ä½“ç»„+ç¿»è¯‘é¢„è§ˆ ==================
    {
        "name": "ç¤¾äº¤åª’ä½“",
        "urls": [
        #    'https://rsshub.app/twitter/media/clawcloud43609', # claw.cloud
         #   'https://rsshub.app/twitter/media/ElonMuskAOC',   # Elon Musk
        #    'https://rsshub.app/twitter/media/elonmusk',   # Elon Musk
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQeRaTukNYft1_6AZPACnog',  # Asmongold
        ],
        "group_key": "FIFTH_RSS_FEEDS",
        "interval": 17990,    # 5å°æ—¶
        "history_days": 300,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("YOUTUBE_RSS"), 
        "processor": {
            "translate": True,
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
         #   "template": "*{subject}*\nğŸ”— {url}",
            "template": "*{subject}*\n[more]({url})",
            "preview": True,        # é¢„è§ˆ
            "show_count": False     #è®¡æ•°
        }
    },
    # ================== æ–°æµªåšå®¢ ==================
    {
        "name": "ç¤¾äº¤åª’ä½“",
        "urls": [
            'https://rsshub.app/weibo/user/3194547262',  # æ±Ÿè¥¿é«˜é€Ÿ
         #   'https://rsshub.app/weibo/user/1699432410',  # æ–°åç¤¾
        #    'https://rsshub.app/weibo/user/2656274875',  # å¤®è§†æ–°é—»
            'https://rsshub.app/weibo/user/2716786595',  # èšèä¹¡
            'https://rsshub.app/weibo/user/1891035762',  # äº¤è­¦
       #     'https://rsshub.app/weibo/user/3917937138',  # å‘å¸ƒ
        #    'https://rsshub.app/weibo/user/3213094623',  # é‚®æ”¿
        #    'https://rsshub.app/weibo/user/2818241427',  # å†’é™©å²›

        
        ],
        "group_key": "FIFTH_RSSSA_FEEDS",
        "interval": 10800,    # 3å°æ—¶ 
        "history_days": 300,     # æ–°å¢ï¼Œä¿ç•™300å¤©
        "bot_token": os.getenv("RRSS_LINDA"), 
        "processor": {
            "translate": False,
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
         #   "template": "*{subject}*\nğŸ”— {url}",
            "template": "*{subject}*\n[more]({url})",
            "preview": False,        # é¢„è§ˆ
            "show_count": False     #è®¡æ•°
        }
    },

    # ================== æŠ€æœ¯è®ºå›ç»„ ==================
    {
        "name": "æŠ€æœ¯è®ºå›",
        "urls": [
            'https://rss.nodeseek.com',  # Nodeseek
        ],
        "group_key": "FIFTH_RSS_RSS_SAN",
        "interval": 290,       # 4åˆ†é’Ÿ 
        "history_days": 3,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_SAN"),
        "processor": {
            "translate": False,                  #ç¿»è¯‘å¼€å…³
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "filter": {
                "enable": False,  # è¿‡æ»¤å¼€å…³     False: å…³é—­ / True: å¼€å¯
                "mode": "allow",  # allowæ¨¡å¼ï¼šåŒ…å«å…³é”®è¯æ‰å‘é€ / blockæ¨¡å¼ï¼šåŒ…å«å…³é”®è¯ä¸å‘é€
                "keywords": ["å…", "cf", "cl", "é»‘", "ä½", "å°", "å¡", "å¹´", "bug", "ç™½", "github",  "èŠ‚",  "é—ª",  "cc", "rn", "åŠ¨", "cloudcone", "docker", "æŠ˜"]  # æœ¬ç»„å…³é”®è¯åˆ—è¡¨
            },
            "preview": False,               # é¢„è§ˆ
            "show_count": False               #è®¡æ•°
        }
    },
    {
        "name": "ç¤¾äº¤åª’ä½“",
        "urls": [
        #    'https://lowendspirit.com/discussions/feed.rss', # lowendspirit
       #     'https://lowendtalk.com/discussions/feed.rss',   # lowendtalk
     
        ],
        "group_key": "FIFTHHHH_RSSS_FEEDS",
        "interval": 7190,      # 1å°æ—¶56åˆ†é’Ÿ
        "history_days": 30,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_SAN"), 
        "processor": {
            "translate": True,
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
         #   "template": "*{subject}*\nğŸ”— {url}",
            "template": "*{subject}*\n[more]({url})",
            "preview": False,        # é¢„è§ˆ
            "show_count": False     #è®¡æ•°
        }
    },
    # ================== YouTubeé¢‘é“ç»„ ==================
    {
        "name": "YouTubeé¢‘é“",
        "urls": [
         #   'https://blog.090227.xyz/atom.xml',
         #   'https://www.freedidi.com/feed',
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCvijahEyGtvMpmMHBu4FS2w', # é›¶åº¦è§£è¯´
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC96OvMh0Mb_3NmuE8Dpu7Gg', # ææœºé›¶è·ç¦»
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQoagx4VHBw3HkAyzvKEEBA', # ç§‘æŠ€å…±äº«
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCbCCUH8S3yhlm7__rhxR2QQ', # ä¸è‰¯æ—
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCMtXiCoKFrc2ovAGc1eywDg', # ä¸€ä¼‘
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCii04BCvYIdQvshrdNDAcww', # æ‚Ÿç©ºçš„æ—¥å¸¸
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCJMEiNh1HvpopPU3n9vJsMQ', # ç†ç§‘ç”·å£«
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCYjB6uufPeHSwuHs8wovLjg', # ä¸­æŒ‡é€š
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCSs4A6HYKmHA2MG_0z-F0xw', # ææ°¸ä¹è€å¸ˆ
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCZDgXi7VpKhBJxsPuZcBpgA', # å¯æ©KeEn
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCxukdnZiXnTFvjF5B5dvJ5w', # ç”¬å“¥ä¾ƒä¾ƒä¾ƒygkkk
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCUfT9BAofYBKUTiEVrgYGZw', # ç§‘æŠ€åˆ†äº«
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC51FT5EeNPiiQzatlA2RlRA', # ä¹Œå®¢wuke
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCDD8WJ7Il3zWBgEYBUtc9xQ', # jack stone
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCWurUlxgm7YJPPggDz9YJjw', # ä¸€ç“¶å¥¶æ²¹
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCvENMyIFurJi_SrnbnbyiZw', # é…·å‹ç¤¾
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCmhbF9emhHa-oZPiBfcLFaQ', # WenWeekly
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC3BNSKOaphlEoK4L7QTlpbA', # ä¸­å¤–è§‚å¯Ÿ
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCXk0rwHPG9eGV8SaF2p8KUQ', # çƒé´‰ç¬‘ç¬‘
                    # ... å…¶ä»–YouTubeé¢‘é“ï¼ˆå…±18ä¸ªï¼‰
        ],
        "group_key": "YOUTUBE_RSSS_FEEDS",
        "interval": 3590,      # 55åˆ†é’Ÿ
        "history_days": 360,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_TOKEN"),
        "processor": {
            "translate": False,
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": True,                # é¢„è§ˆ
            "show_count": False               #è®¡æ•°
        }
    },

    # ================== ä¸­æ–‡YouTubeç»„ ==================
    {
        "name": "ä¸­æ–‡YouTube",
        "urls": [
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCUNciDq-y6I6lEQPeoP-R5A', # è‹æ’è§‚å¯Ÿ
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCXkOTZJ743JgVhJWmNV8F3Q', # å¯’åœ‹äºº
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC2r2LPbOUssIa02EbOIm7NA', # æ˜Ÿçƒç†±é»
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCF-Q1Zwyn9681F7du8DMAWg', # è¬å®—æ¡“-è€è¬ä¾†äº†
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCSYBgX9pWGiUAcBxjnj6JCQ', # éƒ­æ­£äº®é »é“
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCNiJNzSkfumLB7bYtXcIEmg', # çœŸçš„å¾ˆåšé€š
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCN0eCImZY6_OiJbo8cy5bLw', # å±ˆæ©ŸTV
         #   'https://www.youtube.com/feeds/videos.xml?channel_id=UCb3TZ4SD_Ys3j4z0-8o6auA', # BBC News ä¸­æ–‡
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCiwt1aanVMoPYUt_CQYCPQg', # å…¨çƒå¤§è¦–é‡
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC000Jn3HGeQSwBuX_cLDK8Q', # æˆ‘æ˜¯æŸ³å‚‘å…‹
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQFEBaHCJrHu2hzDA_69WQg', # å›½æ¼«è¯´
            'https://www.youtube.com/feeds/videos.xml?channel_id=UChJ8YKw6E1rjFHVS9vovrZw', # BNE TV - æ–°è¥¿å…°ä¸­æ–‡å›½é™…é¢‘é“
          #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCJncdiH3BQUBgCroBmhsUhQ', # è§‚å¯Ÿè€…ç½‘
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCSYBgX9pWGiUAcBxjnj6JCQ', # éƒ­æ­£äº®é »é“
        # å½±è§†
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC7Xeh7thVIgs_qfTlwC-dag', # Marc TV
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCCD14H7fJQl3UZNWhYMG3Mg', # æ¸©åŸé²¤
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQO2T82PiHCYbqmCQ6QO6lw', # æœˆäº®èªª
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCHW6W9g2TJL2_Lf7GfoI5kg', # ç”µå½±æ”¾æ˜ å…
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCi2GvcaxZCN-61a0co8Smnw', # é¤¨é•·
   # bilibili
       #     'https://rsshub.app/bilibili/user/video/271034954', #æ— é™æµ·å­
        #    'https://rsshub.app/bilibili/user/video/10720688', #ä¹Œå®¢wuke
         #   'https://rsshub.app/bilibili/user/video/33683045', #å¼ å¬å¿ 
        #    'https://rsshub.app/bilibili/user/video/9458053', #ææ°¸ä¹
         #   'https://rsshub.app/bilibili/user/video/456664753', #å¤®è§†æ–°é—»
          #  'https://rsshub.app/bilibili/user/video/95832115', #æ±æœµæ›¼
          #  'https://rsshub.app/bilibili/user/video/3546741104183937', #æ²¹ç®¡ç²¾é¸å­—å¹•ç»„
          #  'https://rsshub.app/bilibili/user/video/52165725', #ç‹éªAlbert

            
            
        ],
        "group_key": "FIFTH_RSS_YOUTUBE",
        "interval": 35990,     # 10å°æ—¶
        "history_days": 360,     # æ–°å¢ï¼Œä¿ç•™300å¤©
        "bot_token": os.getenv("YOUTUBE_RSS"),
        "processor": {
        "translate": False,                    #ç¿»è¯‘å¼€å…³
        "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
    #   "template": "*{subject}*\nğŸ”— {url}",
        "template": "*{subject}*\n[more]({url})",
        "preview": True,                       # é¢„è§ˆ
        "show_count": False                    #è®¡æ•°
    }
    },

    # ================== ä¸­æ–‡åª’ä½“ç»„ ==================
    {
        "name": "ä¸­æ–‡åª’ä½“", 
        "urls": [
            'https://rsshub.app/guancha/headline',
            'https://rsshub.app/guancha',
            'https://rsshub.app/zaobao/znews/china',
        ],
        "group_key": "THIRD_RSS_FEEDS",
        "interval": 6990,      # 1å°æ—¶56åˆ†é’Ÿ
        "history_days": 30,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_LINDA_YOUTUBE"),
        "processor": {
            "translate": False,                        #ç¿»è¯‘å¼€å…³
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": False,                              # é¢„è§ˆ
            "show_count": False                       #è®¡æ•°
        }
    }
]

# æ–°å¢é€šç”¨å¤„ç†å‡½æ•°
async def process_group(session, group_config, global_status):
    """ç»Ÿä¸€å¤„ç†RSSç»„ï¼ˆç¡®ä¿å‘é€æˆåŠŸåæ‰ä¿å­˜çŠ¶æ€ï¼Œæ‰€æœ‰çŠ¶æ€éƒ½ç”¨canonical_urlï¼‰"""
    group_name = group_config["name"]
    group_key = group_config["group_key"]
    processor = group_config["processor"]
    bot_token = group_config["bot_token"]

    try:
        # ========== 1. æ£€æŸ¥æ—¶é—´é—´éš” ==========
        last_run = await load_last_run_time_from_db(group_key)
        now = datetime.now(pytz.utc).timestamp()
        if (now - last_run) < group_config["interval"]:
            return  # æœªåˆ°é—´éš”æ—¶é—´ï¼Œè·³è¿‡å¤„ç†

        bot = Bot(token=bot_token)

        # ========== 2. å¤„ç†æ¯ä¸ªURLæº ==========
        for index, feed_url in enumerate(group_config["urls"]):
            try:
                # ===== 2.0 æºé—´å»¶è¿Ÿ =====
                if index > 0:
                    await asyncio.sleep(1)  # æºé—´å»¶è¿Ÿ1ç§’

                # ------ 2.1 è·å–Feedæ•°æ® ------
                feed_data, canonical_url = await fetch_feed(session, feed_url)
                if not feed_data or not feed_data.entries:
                    continue
                processed_ids = global_status.get(canonical_url, set())

                # ------ 2.2 åŠ è½½å¤„ç†çŠ¶æ€ & æ”¶é›†æ–°æ¡ç›® ------
                new_entries = []
                pending_entry_ids = []
                seen_in_batch = set()

                for entry in feed_data.entries:
                    entry_id = get_entry_identifier(entry)
                    if entry_id in processed_ids or entry_id in seen_in_batch:
                        continue
                    seen_in_batch.add(entry_id)

                    # å…³é”®è¯è¿‡æ»¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    filter_config = processor.get("filter", {})
                    if filter_config.get("enable", False):
                        raw_title = remove_html_tags(entry.title or "")
                        keywords = filter_config.get("keywords", [])
                        match = any(kw.lower() in raw_title.lower() for kw in keywords)
                        if filter_config.get("mode", "allow") == "allow":
                            if not match:
                                continue
                        else:
                            if match:
                                continue

                    new_entries.append(entry)
                    pending_entry_ids.append(entry_id)

                # ===== 2.3 å‘é€æ¶ˆæ¯ï¼ˆæˆåŠŸåä¿å­˜çŠ¶æ€ï¼‰ =====
                if new_entries:
                    feed_message = await generate_group_message(feed_data, new_entries, processor)
                    if feed_message:
                        try:
                            await send_single_message(
                                bot,
                                TELEGRAM_CHAT_ID[0],
                                feed_message,
                                disable_web_page_preview=not processor.get("preview", True)
                            )
                            for entry_id in pending_entry_ids:
                                await save_single_status(group_key, canonical_url, entry_id)
                                processed_ids.add(entry_id)

                            global_status[canonical_url] = processed_ids

                        except Exception as send_error:
                            logger.error(f"âŒ å‘é€æ¶ˆæ¯å¤±è´¥ [{feed_url}]")
                            raise

            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¤±è´¥ [{feed_url}]")

        # ========== 3. ä¿å­˜æœ€åè¿è¡Œæ—¶é—´ ==========
        await save_last_run_time_to_db(group_key, now)
    except Exception as e:
        logger.critical(f"â€¼ï¸ å¤„ç†ç»„å¤±è´¥ [{group_key}]")

async def generate_group_message(feed_data, entries, processor):
    """ç”Ÿæˆæ ‡å‡†åŒ–æ¶ˆæ¯å†…å®¹"""
    try:
        # ===== 1. åŸºç¡€ä¿¡æ¯å¤„ç† =====
        source_name = feed_data.feed.get('title', "æœªçŸ¥æ¥æº")
        safe_source = escape(source_name)
        
        # ===== æ–°å¢ï¼šæ ‡é¢˜å¤„ç† =====
        header = ""
        if "header_template" in processor:
            header = processor["header_template"].format(source=safe_source) + "\n"
        
        messages = []

        # ===== 2. å¤„ç†æ¯ä¸ªæ¡ç›® =====
        for entry in entries:
            # -- 2.1 æ ‡é¢˜å¤„ç† --
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
            if processor["translate"]:
                translated_subject = await auto_translate_text(raw_subject)
            else:
                translated_subject = raw_subject
            safe_subject = escape(translated_subject)

            # -- 2.2 é“¾æ¥å¤„ç† --
            raw_url = entry.link
            safe_url = escape(raw_url)

            # -- 2.3 æ„å»ºæ¶ˆæ¯ --
            message = processor["template"].format(
                subject=safe_subject,
                source=safe_source,
                url=safe_url
            )
            messages.append(message)

        full_message = header + "\n\n".join(messages)
        
        if processor.get("show_count", True):
            full_message += f"\n\nâœ… æ–°å¢ {len(messages)} æ¡å†…å®¹"
            
        return full_message
    
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ¶ˆæ¯å¤±è´¥: {str(e)}")
        return ""

def create_connection():
    """åˆ›å»º SQLite æ•°æ®åº“è¿æ¥"""
    conn = None
    try:
        conn = sqlite3.connect(DATABASE_FILE)
        return conn
    except sqlite3.Error as e:
        logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
    return conn

def create_table():
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            # ä»…ä¿ç•™SQLiteå»ºè¡¨è¯­å¥
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS rss_status (
                    feed_group TEXT,
                    feed_url TEXT,
                    entry_url TEXT,
                    entry_timestamp REAL,
                    PRIMARY KEY (feed_group, feed_url, entry_url)
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_run_time REAL
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS cleanup_timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_cleanup_time REAL
                )
            """)
            conn.commit()
        except sqlite3.Error as e:
            logger.error(f"åˆ›å»ºæœ¬åœ°è¡¨å¤±è´¥: {e}")
        finally:
            conn.close()

async def load_last_run_time_from_db(feed_group):
    """ä»…ä½¿ç”¨SQLiteåŠ è½½æ—¶é—´æˆ³"""
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT last_run_time FROM timestamps WHERE feed_group = ?", (feed_group,))
            result = cursor.fetchone()
            return result[0] if result else 0
        except sqlite3.Error as e:
            logger.error(f"ä»æœ¬åœ°æ•°æ®åº“åŠ è½½æ—¶é—´å¤±è´¥: {e}")
            return 0
        finally:
            conn.close()
    return 0

async def save_last_run_time_to_db(feed_group, last_run_time):
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("BEGIN TRANSACTION")
            cursor.execute("""
                INSERT OR REPLACE INTO timestamps (feed_group, last_run_time)
                VALUES (?, ?)
            """, (feed_group, last_run_time))
            conn.commit()
        except sqlite3.Error as e:
            logger.error(f"æ—¶é—´æˆ³ä¿å­˜å¤±è´¥: {e}")
            conn.rollback()
        finally:
            conn.close()

def remove_html_tags(text):
    text = re.sub(r'#([^#\s]+)#', r'\1', text)  # åŒ¹é… #æ–‡å­—# â†’ æ–‡å­—
    text = re.sub(r'#\w+', '', text)    # ç§»é™¤ hashtags
    text = re.sub(r'@[^\s]+', '', text).strip()     # ç§»é™¤ @æåŠ
    text = re.sub(r'ã€\s*ã€‘', '', text)    # ç§»é™¤ ã€ã€‘ç¬¦å·ï¼ˆå«ä¸­é—´ç©ºæ ¼ï¼‰
    text = re.sub(r'(?<!\S)#(?!\S)', '', text)
    text = re.sub(r'(?<!\S)ï¼š(?!\S)', '', text)
    return text


@retry(
    stop=stop_after_attempt(1),
    wait=wait_exponential(multiplier=1, min=5, max=30),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def send_single_message(bot, chat_id, text, disable_web_page_preview=False):
    try:
        MAX_MESSAGE_LENGTH = 4096
        text_chunks = []
        current_chunk = []
        current_length = 0

        # æŒ‰æ®µè½åˆ†å‰²ä¿æŒç»“æ„
        paragraphs = text.split('\n\n')
        for para in paragraphs:
            para_length = len(para.encode('utf-8'))
            if current_length + para_length + 2 > MAX_MESSAGE_LENGTH:
                text_chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(para)
            current_length += para_length + 2

        if current_chunk:
            text_chunks.append('\n\n'.join(current_chunk))

        # v20.x çš„æ­£ç¡®å‚æ•°
        for chunk in text_chunks:
            await bot.send_message(
                chat_id=chat_id,
                text=chunk,
                parse_mode='MarkdownV2',
                disable_web_page_preview=disable_web_page_preview,
                read_timeout=10,  # è¯»å–è¶…æ—¶
                write_timeout=10  # å†™å…¥è¶…æ—¶
            )
    except BadRequest as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥(Markdowné”™è¯¯): {e} - æ–‡æœ¬ç‰‡æ®µ: {chunk[:200]}...")
    except Exception as e:
        raise

@retry(
    stop=stop_after_attempt(1),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def fetch_feed(session, feed_url):
    """
    è·å–RSS Feedæ•°æ®ï¼Œåªæœ‰rsshub.appä¸»åŸŸåæ‰ç”¨å¤‡ç”¨åŸŸåï¼Œå¦åˆ™åªç”¨åŸå§‹åŸŸåã€‚
    è¿”å›: feed_data, canonical_url
    """
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}
    parsed = urlparse(feed_url)
    is_rsshub = parsed.netloc == "rsshub.app"  # åªå…è®¸ä¸»åŸŸåä½¿ç”¨å¤‡ç”¨åŸŸå

    if is_rsshub:
        try_domains = BACKUP_DOMAINS + ["rsshub.app"]
        canonical_url = feed_url.replace(parsed.netloc, "rsshub.app")
    else:
        try_domains = [parsed.netloc]
        canonical_url = feed_url
        
    for domain in try_domains:
        modified_url = feed_url.replace(parsed.netloc, domain)
        try:
            async with semaphore:
                async with session.get(modified_url, headers=headers, timeout=30) as response:
                    if response.status in (503, 403, 404, 429):
                        continue  # å°è¯•ä¸‹ä¸€ä¸ªåŸŸå
                    response.raise_for_status()
                    return parse(await response.read()), canonical_url
        except aiohttp.ClientResponseError as e:
            if e.status in (503, 403, 404, 429):
                continue
        except Exception as e:
            logger.error(f"è¯·æ±‚å¤±è´¥: {modified_url}, é”™è¯¯: {e}")
            continue
    logger.error(f"æ‰€æœ‰åŸŸåå°è¯•å¤±è´¥: {feed_url}")
    return None, canonical_url

async def translate_with_credentials(secret_id, secret_key, text):
    loop = asyncio.get_running_loop()
    text_bytes = text.encode('utf-8')
    if len(text_bytes) > 2000:
        safe_bytes = text_bytes[:2000]
        while safe_bytes[-1] & 0xC0 == 0x80:
            safe_bytes = safe_bytes[:-1]
        text = safe_bytes.decode('utf-8', errors='ignore')
        logger.warning(f"æ–‡æœ¬æˆªæ–­è‡³ {len(text)} å­—ç¬¦ ({len(safe_bytes)} å­—èŠ‚)")
    try:
        return await loop.run_in_executor(
            None, 
            lambda: _sync_translate(secret_id, secret_key, text)
        )
    except Exception as e:
        logger.error(f"ç¿»è¯‘æ‰§è¡Œå¤±è´¥: {type(e).__name__} - {str(e)}")
        raise

def _sync_translate(secret_id, secret_key, text):
    try:
        cred = credential.Credential(secret_id, secret_key)
        clientProfile = ClientProfile(httpProfile=HttpProfile(endpoint="tmt.tencentcloudapi.com"))
        client = tmt_client.TmtClient(cred, TENCENT_REGION, clientProfile)
        req = models.TextTranslateRequest()
        req.SourceText = remove_html_tags(text)
        req.Source = "auto"
        req.Target = "zh"
        req.ProjectId = 0
        return client.TextTranslate(req).TargetText
    except TencentCloudSDKException as e:
        error_details = {
            "code": e.code,
            "message": e.message,
            "request_id": e.request_id,
            "region": TENCENT_REGION
        }
        logger.error(f"è…¾è®¯äº‘APIé”™è¯¯è¯¦æƒ…: {error_details}")
        raise
    except Exception as e:
        logger.error(f"ç¿»è¯‘è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        raise

@retry(
    stop=stop_after_attempt(2),
    wait=wait_exponential(multiplier=1, min=2, max=10),
)
async def auto_translate_text(text):
    try:
        try:
            return await translate_with_credentials(
                TENCENTCLOUD_SECRET_ID, 
                TENCENTCLOUD_SECRET_KEY,
                text
            )
        except TencentCloudSDKException as e:
            logger.error(f"ä¸»å¯†é’¥ç¿»è¯‘å¤±è´¥: [Code: {e.code}] {e.message}")
            raise
        except Exception as e:
            logger.error(f"ä¸»å¯†é’¥ç¿»è¯‘æœªçŸ¥é”™è¯¯: {type(e).__name__} - {str(e)}")
            raise
    except Exception as first_error:
        if TENCENT_SECRET_ID and TENCENT_SECRET_KEY:
            logger.warning("ä¸»ç¿»è¯‘å¯†é’¥å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨å¯†é’¥...")
            try:
                return await translate_with_credentials(
                    TENCENT_SECRET_ID,
                    TENCENT_SECRET_KEY,
                    text
                )
            except TencentCloudSDKException as e:
                logger.error(f"å¤‡ç”¨å¯†é’¥ç¿»è¯‘å¤±è´¥: [Code: {e.code}] {e.message}")
                raise
            except Exception as e:
                logger.error(f"å¤‡ç”¨å¯†é’¥ç¿»è¯‘æœªçŸ¥é”™è¯¯: {type(e).__name__} - {str(e)}")
                raise
        else:
            logger.error("ä¸»ç¿»è¯‘å¯†é’¥å¤±è´¥ï¼Œä¸”æœªé…ç½®å¤‡ç”¨å¯†é’¥")
            raise first_error
    except Exception as final_error:
        logger.error(f"æ‰€æœ‰ç¿»è¯‘å°è¯•å‡å¤±è´¥: {type(final_error).__name__}")
        cleaned = remove_html_tags(text)
        return escape(cleaned)

async def load_status():
    status = {}
    conn = create_connection()
    if conn is not None:
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT feed_url, entry_url FROM rss_status")
            for feed_url, entry_url in cursor.fetchall():
                if feed_url not in status:
                    status[feed_url] = set()
                status[feed_url].add(entry_url)
        except sqlite3.Error as e:
            logger.error(f"æœ¬åœ°çŠ¶æ€åŠ è½½å¤±è´¥: {e}")
        finally:
            conn.close()
    return status

async def save_single_status(feed_group, feed_url, entry_url):
    timestamp = time.time()
    max_retries = 3
    for attempt in range(max_retries):
        conn = create_connection()
        if conn:
            try:
                cursor = conn.cursor()
                cursor.execute("BEGIN TRANSACTION")
                cursor.execute("""
                    INSERT OR IGNORE INTO rss_status 
                    VALUES (?, ?, ?, ?)
                """, (feed_group, feed_url, entry_url, timestamp))
                conn.commit()
                return
            except sqlite3.OperationalError as e:
                if "locked" in str(e) and attempt < max_retries - 1:
                    await asyncio.sleep(0.1 * (attempt + 1))
                    continue
                logger.error(f"SQLiteä¿å­˜å¤±è´¥ï¼ˆå°è¯•{attempt+1}æ¬¡ï¼‰: {e}")
            except sqlite3.Error as e:
                logger.error(f"SQLiteé”™è¯¯: {e}")
            finally:
                conn.close()

def get_entry_identifier(entry):
    if hasattr(entry, 'guid') and entry.guid:
        return hashlib.sha256(entry.guid.encode()).hexdigest()
    link = getattr(entry, 'link', '')
    if link:
        try:
            parsed = urlparse(link)
            clean_link = parsed._replace(query=None, fragment=None).geturl().lower()
            return hashlib.sha256(clean_link.encode()).hexdigest()
        except Exception as e:
            logger.warning(f"URLè§£æå¤±è´¥ {link}: {e}")
    title = getattr(entry, 'title', '')
    pub_date = get_entry_timestamp(entry).isoformat() if get_entry_timestamp(entry) else ''
    return hashlib.sha256(f"{title}|||{pub_date}".encode()).hexdigest()

def get_entry_timestamp(entry):
    dt = datetime.now(pytz.UTC)
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        dt = datetime(*entry.published_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'pubDate_parsed') and entry.pubDate_parsed:
        dt = datetime(*entry.pubDate_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        dt = datetime(*entry.updated_parsed[:6], tzinfo=pytz.utc)
    return dt

async def process_feed_common(session, feed_group, feed_url, status):
    try:
        feed_data = await fetch_feed(session, feed_url)
        if not feed_data or not feed_data.entries:
            return None
        processed_ids = status.get(feed_url, set())
        new_entries = []
        for entry in feed_data.entries:
            entry_id = get_entry_identifier(entry)
            if entry_id in processed_ids:
                continue
            new_entries.append(entry)
            await save_single_status(feed_group, feed_url, entry_id)
            processed_ids.add(entry_id)
        status[feed_url] = processed_ids  # æ›´æ–°å†…å­˜çŠ¶æ€
        return feed_data, new_entries
    except Exception as e:
        return None

def cleanup_history(days, feed_group):
    conn = create_connection()
    if conn:
        try:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT last_cleanup_time FROM cleanup_timestamps WHERE feed_group = ?", 
                (feed_group,)
            )
            result = cursor.fetchone()
            last_cleanup = result[0] if result else 0
            now = time.time()
            if now - last_cleanup < 86400:
                return
            cutoff_ts = now - days * 86400
            cursor.execute(
                "DELETE FROM rss_status WHERE feed_group=? AND entry_timestamp < ?",
                (feed_group, cutoff_ts)
            )
            affected_rows = cursor.rowcount
            cursor.execute("""
                INSERT OR REPLACE INTO cleanup_timestamps (feed_group, last_cleanup_time)
                VALUES (?, ?)
            """, (feed_group, now))
            conn.commit()
        except sqlite3.Error as e:
            logger.error(f"âŒ æ—¥å¿—æ¸…ç†å¤±è´¥: ç»„={feed_group}, é”™è¯¯={e}")
        finally:
            conn.close()

def signal_handler(signum, frame):
    logger.warning(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œç¨‹åºå³å°†é€€å‡ºã€‚")
    sys.exit(0)

async def main():
    lock_file = None
    try:
        lock_file = open(LOCK_FILE, "w")
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
    except OSError:
        logger.warning("â›” æ— æ³•è·å–æ–‡ä»¶é”ï¼Œå·²æœ‰å®ä¾‹åœ¨è¿è¡Œï¼Œç¨‹åºé€€å‡º")
        return
    except Exception as e:
        logger.critical(f"â€¼ï¸ æ–‡ä»¶é”å¼‚å¸¸: {str(e)}")
        return
    try:
        create_table()
    except Exception as e:
        logger.critical(f"â€¼ï¸ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return
    for group in RSS_GROUPS:
        days = group.get("history_days", 30)
        try:
            cleanup_history(days, group["group_key"])
        except Exception as e:
            logger.error(f"æ¸…ç†å†å²è®°å½•å¼‚å¸¸: ç»„={group['group_key']}, é”™è¯¯={e}")
    async with aiohttp.ClientSession() as session:
        try:
            status = await load_status()
            tasks = []
            for group in RSS_GROUPS:
                try:
                    tasks.append(process_group(session, group, status))
                except Exception as e:
                    logger.error(f"âš ï¸ åˆ›å»ºä»»åŠ¡å¤±è´¥ [{group['name']}]: {str(e)}")
            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                for res in results:
                    if isinstance(res, Exception):
                        logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {res}")
            else:
                logger.warning("â›” æœªåˆ›å»ºä»»ä½•å¤„ç†ä»»åŠ¡")
        except asyncio.CancelledError:
            logger.warning("â¹ï¸ ä»»åŠ¡è¢«å–æ¶ˆ")
        except Exception as e:
            logger.critical(f"â€¼ï¸ ä¸»å¾ªç¯å¼‚å¸¸: {str(e)}", exc_info=True)
        finally:
            try:
                await session.close()
            except Exception as e:
                logger.error(f"âš ï¸ å…³é—­ä¼šè¯å¤±è´¥: {str(e)}")
    try:
        if lock_file:
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
            lock_file.close()
    except Exception as e:
        logger.error(f"âš ï¸ é‡Šæ”¾æ–‡ä»¶é”å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    for s in (signal.SIGINT, signal.SIGTERM):
        signal.signal(s, signal_handler)
    create_table()
    try:
        asyncio.run(main())
    except Exception as e:
        logger.critical(f"â€¼ï¸ ä¸»è¿›ç¨‹æœªæ•è·å¼‚å¸¸: {str(e)}", exc_info=True)
        sys.exit(1)