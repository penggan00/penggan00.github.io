import asyncio
import aiohttp
import logging
import re
import os
import hashlib
import pytz
import fcntl
import time
import signal
import sys
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from feedparser import parse
from telegram import Bot
from telegram.error import BadRequest
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from md2tgmd import escape
from tencentcloud.common import credential
from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from tencentcloud.tmt.v20180321 import tmt_client, models
from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException

# ========== ç¯å¢ƒåŠ è½½ ==========
load_dotenv()
BASE_DIR = Path(__file__).resolve().parent
LOCK_FILE = BASE_DIR / "rss.lock"
DATABASE_FILE = BASE_DIR / "rss.db"

logging.basicConfig(
    filename=BASE_DIR / "rss.log",
    level=logging.WARNING,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)
logger = logging.getLogger(__name__)

TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID").split(",")
TENCENTCLOUD_SECRET_ID = os.getenv("TENCENTCLOUD_SECRET_ID")
TENCENTCLOUD_SECRET_KEY = os.getenv("TENCENTCLOUD_SECRET_KEY")
TENCENT_REGION = os.getenv("TENCENT_REGION", "na-siliconvalley")
TENCENT_SECRET_ID = os.getenv("TENCENT_SECRET_ID")
TENCENT_SECRET_KEY = os.getenv("TENCENT_SECRET_KEY")
semaphore = asyncio.Semaphore(2)
BACKUP_DOMAINS_STR = os.getenv("BACKUP_DOMAINS", "")
BACKUP_DOMAINS = [domain.strip() for domain in BACKUP_DOMAINS_STR.split(",") if domain.strip()]

RSS_GROUPS = [ # RSS ç»„é…ç½®åˆ—è¡¨
    # ================== å›½é™…æ–°é—»ç»„ ==================False: å…³é—­ / True: å¼€å¯
    {
        "name": "å›½é™…æ–°é—»",
        "urls": [
            'https://feeds.bbci.co.uk/news/world/rss.xml',  # BBC
            'https://www3.nhk.or.jp/rss/news/cat6.xml',     # NHK
       #     'https://www.cnbc.com/id/100003114/device/rss/rss.html',  # CNBC
         #   'https://feeds.a.dj.com/rss/RSSWorldNews.xml',  # åå°”è¡—æ—¥æŠ¥
        #    'https://feeds.content.dowjones.io/public/rss/RSSWorldNews',   # åå°”è¡—æ—¥æŠ¥
        #    'https://feeds.content.dowjones.io/public/rss/socialeconomyfeed',
           'https://www.aljazeera.com/xml/rss/all.xml',    # åŠå²›ç”µè§†å°
        #    'https://www.ft.com/?format=rss',                 # é‡‘èæ—¶æŠ¥
       #     'https://www3.nhk.or.jp/rss/news/cat5.xml',  # NHK å•†ä¸š
       #     'http://rss.cnn.com/rss/cnn_topstories.rss',   # cnn
       #     'https://www.theguardian.com/world/rss',     # å«æŠ¥
      #      'https://www.theverge.com/rss/index.xml',   # The Verge:
        ],
        "group_key": "RSS_FEEDS",
        "interval": 3590,      # 60åˆ†é’Ÿ 
        "batch_send_interval": 14390,   # æ‰¹é‡æ¨é€
        "history_days": 30,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_TWO"),    # Telegram Bot Token
        "processor": {
            "translate": True,       #ç¿»è¯‘å¼€
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": False,         # ç¦æ­¢é¢„è§ˆ
            "show_count": False        # âœ…æ–°å¢
        }
    },

    # ================== å›½é™…æ–°é—»ä¸­æ–‡ç»„ ==================False: å…³é—­ / True: å¼€å¯
    {
        "name": "å›½é™…æ–°é—»ä¸­æ–‡",
        "urls": [
            'https://www.ftchinese.com/rss/news',   # ftä¸­æ–‡ç½‘
        ],
        "group_key": "RSS_FEEDS_INTERNATIONAL",
        "interval": 7200,      # 2å°æ—¶
        "batch_send_interval": 35990,   # æ‰¹é‡æ¨é€â†åŠ ä¸Šå³
        "history_days": 300,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_TWO"),    # Telegram Bot Token
        "processor": {
            "translate": False,       #ç¿»è¯‘ False: å…³é—­ / True: å¼€å¯
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": False,         # ç¦æ­¢é¢„è§ˆ
            "show_count": False        # âœ…æ–°å¢
        }
    },

    # ================== å¿«è®¯ç»„ ==================
    {
        "name": "å¿«è®¯",
        "urls": [
         #   'https://rsshub.app/10jqka/realtimenews', #åŒèŠ±é¡ºè´¢ç»
            'https://36kr.com/feed-newsflash',  # 36æ°ªå¿«è®¯
        #    'https://36kr.com/feed',  # 36æ°ªç»¼åˆ
            
        ],
        "group_key": "FOURTH_RSS_FEEDS",
        "interval": 710,       # 10åˆ†é’Ÿ 
        "batch_send_interval": 3590,   # æ‰¹é‡æ¨é€
        "history_days": 3,     # æ–°å¢ï¼Œä¿ç•™3å¤©
        "bot_token": os.getenv("RSS_LINDA"),   # Telegram Bot Token
        "processor": {
            "translate": False,     #ç¿»è¯‘å¼€å…³
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": False,            # ç¦æ­¢é¢„è§ˆ
            "show_count": False          #è®¡æ•°
        }
    },
    # ================== å¿«è®¯ç»„ ==================
    {
        "name": "å¿«è®¯",
        "urls": [
            'https://rsshub.app/10jqka/realtimenews', #åŒèŠ±é¡ºè´¢ç»
        #    'https://rsshub.app/eastmoney/report/strategyreport', # ä¸œæ–¹è´¢å¯Œç­–ç•¥
         #   'https://rsshub.app/jin10',  # é‡‘å
       #     'https://rsshub.app/huijin-inv/news',
       #     'https://rsshub.app/eeo/kuaixun',
         #   'https://36kr.com/feed-newsflash',  # 36æ°ªå¿«è®¯
        #    'https://36kr.com/feed',  # 36æ°ªç»¼åˆ
            
        ],
        "group_key": "TOURTH_RSS_FEEDS",
        "interval": 710,       # 15åˆ†é’Ÿ
        "batch_send_interval": 1790,   # æ‰¹é‡æ¨é€
        "history_days": 3,     # æ–°å¢ï¼Œä¿ç•™3å¤©
        "bot_token": os.getenv("TONGHUASHUN_RSS"),  #   Telegram Bot Token
        "processor": {
            "translate": False,     #ç¿»è¯‘å¼€å…³
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": False,            # ç¦æ­¢é¢„è§ˆ
            "show_count": False          #è®¡æ•°
        }
    },

    # ================== æ–°æµªåšå®¢ ==================
    {
        "name": "ç¤¾äº¤åª’ä½“",
        "urls": [
            'https://rsshub.app/weibo/user/3194547262',  # æ±Ÿè¥¿é«˜é€Ÿ
        #    'https://rsshub.app/weibo/user/1699432410',  # æ–°åç¤¾
        #    'https://rsshub.app/weibo/user/2656274875',  # å¤®è§†æ–°é—»
            'https://rsshub.app/weibo/user/2716786595',  # èšèä¹¡
            'https://rsshub.app/weibo/user/1891035762',  # äº¤è­¦
       #     'https://rsshub.app/weibo/user/3917937138',  # å‘å¸ƒ
        #    'https://rsshub.app/weibo/user/3213094623',  # é‚®æ”¿
        #    'https://rsshub.app/weibo/user/2818241427',  # å†’é™©å²›

        ],
        "group_key": "FIFTH_RSSSA_FEEDS",
        "interval": 21590,    # 4å°æ—¶
       # "batch_send_interval": 21590,   # æ‰¹é‡æ¨é€    
        "history_days": 300,     # æ–°å¢ï¼Œä¿ç•™300å¤©
        "bot_token": os.getenv("RRSS_LINDA"),  # Telegram Bot Token
        "processor": {
            "translate": False,     #ç¿»è¯‘å…³
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
         #   "template": "*{subject}*\nğŸ”— {url}",
            "template": "*{subject}*\n[more]({url})",
            "preview": False,        # ç¦æ­¢é¢„è§ˆ
            "show_count": False     #è®¡æ•°
        }
    },

    # ================== æŠ€æœ¯è®ºå›ç»„ ==================
    {
        "name": "æŠ€æœ¯è®ºå›",
        "urls": [
            'https://rss.nodeseek.com',  # Nodeseek  
        ],
        "group_key": "FIFTH_RSS_RSS_SAN", 
        "interval": 240,       # 4åˆ†é’Ÿ 
        "batch_send_interval": 3590,   # æ‰¹é‡æ¨é€
        "history_days": 3,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_SAN"), # Telegram Bot Token
        "processor": {
            "translate": False,                  #ç¿»è¯‘å…³
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})", 
            "filter": {
                "enable": True,  # è¿‡æ»¤å¼€å…³     False: å…³é—­ / True: å¼€å¯
                "mode": "allow",  # allowæ¨¡å¼ï¼šåŒ…å«å…³é”®è¯æ‰å‘é€ / blockæ¨¡å¼ï¼šåŒ…å«å…³é”®è¯ä¸å‘é€
                "keywords": ["å…", "cf", "cl", "é»‘", "ä½", "å°", "å¡", "å¹´", "bug", "ç™½", "github",  "èŠ‚",  "é—ª",  "cc", "rn", "åŠ¨", "cloudcone", "è„šæœ¬", "ä»£ç ", "docker", "å‰©", "æŠ˜"]  # æœ¬ç»„å…³é”®è¯åˆ—è¡¨
            },
            "preview": False,              # ç¦æ­¢é¢„è§ˆ
            "show_count": False               #è®¡æ•°
        }
    },
    # ================== vps ==================
    {
        "name": "vps",
        "urls": [
        #    'https://lowendspirit.com/discussions/feed.rss', # lowendspirit
            'https://lowendtalk.com/discussions/feed.rss',   # lowendtalk
        ],
        "group_key": "FIFTH_RSS_RRSS_SAN",
        "interval": 3300,      # 60åˆ†é’Ÿ 
        "batch_send_interval": 21590,   # æ‰¹é‡æ¨é€
        "history_days": 60,     # ä¿ç•™60å¤©
        "bot_token": os.getenv("RSS_SAN"),    # Telegram Bot Token
        "processor": {
            "translate": True,       #ç¿»è¯‘å¼€
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": False,         # ç¦æ­¢é¢„è§ˆ
            "show_count": False        # âœ…æ–°å¢
        }
    },
    # ================== YouTubeé¢‘é“ç»„ ==================
    {
        "name": "YouTubeé¢‘é“",
        "urls": [
         #   'https://blog.090227.xyz/atom.xml',
         #   'https://www.freedidi.com/feed',
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCvijahEyGtvMpmMHBu4FS2w', # é›¶åº¦è§£è¯´
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC96OvMh0Mb_3NmuE8Dpu7Gg', # ææœºé›¶è·ç¦»
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQoagx4VHBw3HkAyzvKEEBA', # ç§‘æŠ€å…±äº«
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCbCCUH8S3yhlm7__rhxR2QQ', # ä¸è‰¯æ—
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCMtXiCoKFrc2ovAGc1eywDg', # ä¸€ä¼‘
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCii04BCvYIdQvshrdNDAcww', # æ‚Ÿç©ºçš„æ—¥å¸¸
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCJMEiNh1HvpopPU3n9vJsMQ', # ç†ç§‘ç”·å£«
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCYjB6uufPeHSwuHs8wovLjg', # ä¸­æŒ‡é€š
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCSs4A6HYKmHA2MG_0z-F0xw', # ææ°¸ä¹è€å¸ˆ
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCZDgXi7VpKhBJxsPuZcBpgA', # å¯æ©KeEn
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCxukdnZiXnTFvjF5B5dvJ5w', # ç”¬å“¥ä¾ƒä¾ƒä¾ƒygkkk
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCUfT9BAofYBKUTiEVrgYGZw', # ç§‘æŠ€åˆ†äº«
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC51FT5EeNPiiQzatlA2RlRA', # ä¹Œå®¢wuke
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCDD8WJ7Il3zWBgEYBUtc9xQ', # jack stone
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCWurUlxgm7YJPPggDz9YJjw', # ä¸€ç“¶å¥¶æ²¹
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCvENMyIFurJi_SrnbnbyiZw', # é…·å‹ç¤¾
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCmhbF9emhHa-oZPiBfcLFaQ', # WenWeekly
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC3BNSKOaphlEoK4L7QTlpbA', # ä¸­å¤–è§‚å¯Ÿ
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCXk0rwHPG9eGV8SaF2p8KUQ', # çƒé´‰ç¬‘ç¬‘
                    # ... å…¶ä»–YouTubeé¢‘é“ï¼ˆå…±18ä¸ªï¼‰
        ],
        "group_key": "YOUTUBE_RSSS_FEEDS", # YouTubeé¢‘é“
        "interval": 3590,      # 60åˆ†é’Ÿ
       # "batch_send_interval": 10800,   # æ‰¹é‡æ¨é€
        "history_days": 360,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_TOKEN"),   # Telegram Bot Token
        "processor": {
            "translate": False,                    #ç¿»è¯‘å…³
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": True,                # é¢„è§ˆ
            "show_count": False               #è®¡æ•°
        }
    },

    # ================== ä¸­æ–‡YouTubeç»„ ==================
    {
        "name": "ä¸­æ–‡YouTube",
        "urls": [
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCUNciDq-y6I6lEQPeoP-R5A', # è‹æ’è§‚å¯Ÿ
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCXkOTZJ743JgVhJWmNV8F3Q', # å¯’åœ‹äºº
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC2r2LPbOUssIa02EbOIm7NA', # æ˜Ÿçƒç†±é»
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCF-Q1Zwyn9681F7du8DMAWg', # è¬å®—æ¡“-è€è¬ä¾†äº†
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCSYBgX9pWGiUAcBxjnj6JCQ', # éƒ­æ­£äº®é »é“
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCNiJNzSkfumLB7bYtXcIEmg', # çœŸçš„å¾ˆåšé€š
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCN0eCImZY6_OiJbo8cy5bLw', # å±ˆæ©ŸTV
         #   'https://www.youtube.com/feeds/videos.xml?channel_id=UCb3TZ4SD_Ys3j4z0-8o6auA', # BBC News ä¸­æ–‡
       #     'https://www.youtube.com/feeds/videos.xml?channel_id=UCiwt1aanVMoPYUt_CQYCPQg', # å…¨çƒå¤§è¦–é‡
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC000Jn3HGeQSwBuX_cLDK8Q', # æˆ‘æ˜¯æŸ³å‚‘å…‹
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQFEBaHCJrHu2hzDA_69WQg', # å›½æ¼«è¯´
            'https://www.youtube.com/feeds/videos.xml?channel_id=UChJ8YKw6E1rjFHVS9vovrZw', # BNE TV - æ–°è¥¿å…°ä¸­æ–‡å›½é™…é¢‘é“
          #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCJncdiH3BQUBgCroBmhsUhQ', # è§‚å¯Ÿè€…ç½‘
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCSYBgX9pWGiUAcBxjnj6JCQ', # éƒ­æ­£äº®é »é“
        # å½±è§†
            'https://www.youtube.com/feeds/videos.xml?channel_id=UC7Xeh7thVIgs_qfTlwC-dag', # Marc TV
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCCD14H7fJQl3UZNWhYMG3Mg', # æ¸©åŸé²¤
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQO2T82PiHCYbqmCQ6QO6lw', # æœˆäº®èªª
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCHW6W9g2TJL2_Lf7GfoI5kg', # ç”µå½±æ”¾æ˜ å…
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCi2GvcaxZCN-61a0co8Smnw', # é¤¨é•·
   # bilibili
       #     'https://rsshub.app/bilibili/user/video/271034954', #æ— é™æµ·å­
        #    'https://rsshub.app/bilibili/user/video/10720688', #ä¹Œå®¢wuke
         #   'https://rsshub.app/bilibili/user/video/33683045', #å¼ å¬å¿ 
        #    'https://rsshub.app/bilibili/user/video/9458053', #ææ°¸ä¹
         #   'https://rsshub.app/bilibili/user/video/456664753', #å¤®è§†æ–°é—»
          #  'https://rsshub.app/bilibili/user/video/95832115', #æ±æœµæ›¼
          #  'https://rsshub.app/bilibili/user/video/3546741104183937', #æ²¹ç®¡ç²¾é¸å­—å¹•ç»„
          #  'https://rsshub.app/bilibili/user/video/52165725', #ç‹éªAlbert
        ],
        "group_key": "FIFTH_RSS_YOUTUBE", # YouTubeé¢‘é“
        "interval": 7180,     # 2å°æ—¶
        "batch_send_interval": 35990,   # æ‰¹é‡æ¨é€
        "history_days": 360,     # æ–°å¢ï¼Œä¿ç•™300å¤©
        "bot_token": os.getenv("YOUTUBE_RSS"),    # Telegram Bot Token
        "processor": {
        "translate": False,                    #ç¿»è¯‘å…³
        "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
    #   "template": "*{subject}*\nğŸ”— {url}",
        "template": "*{subject}*\n[more]({url})",
        "preview": True,                       # é¢„è§ˆ
        "show_count": False                    #è®¡æ•°
    }
    },
    # ================== ç¤¾äº¤åª’ä½“ç»„+ç¿»è¯‘é¢„è§ˆ ==================
    {
        "name": "ç¤¾äº¤åª’ä½“",
        "urls": [
        #    'https://rsshub.app/twitter/media/clawcloud43609', # claw.cloud
         #   'https://rsshub.app/twitter/media/ElonMuskAOC',   # Elon Musk
        #    'https://rsshub.app/twitter/media/elonmusk',   # Elon Musk
            'https://www.youtube.com/feeds/videos.xml?channel_id=UCQeRaTukNYft1_6AZPACnog',  # Asmongold

        ],
        "group_key": "FIFTH_RSS_FEEDS",   # YouTubeé¢‘é“
        "interval": 7000,    # 2å°æ—¶
        "batch_send_interval": 21590,   # æ‰¹é‡æ¨é€
        "history_days": 300,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("YOUTUBE_RSS"),  # Telegram Bot Token
        "processor": {
            "translate": True,          #ç¿»è¯‘å¼€
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
         #   "template": "*{subject}*\nğŸ”— {url}",
            "template": "*{subject}*\n[more]({url})",
            "preview": True,        # é¢„è§ˆ
            "show_count": False     #è®¡æ•°
        }
    },
    # ================== ä¸­æ–‡åª’ä½“ç»„ ==================
    {
        "name": "ä¸­æ–‡åª’ä½“", 
        "urls": [
            'https://rsshub.app/guancha/headline',
            'https://rsshub.app/guancha',
            'https://rsshub.app/zaobao/znews/china',
        ],
        "group_key": "THIRD_RSS_FEEDS",
        "interval": 7180,      # 2å°æ—¶
        "batch_send_interval": 14380,   # æ‰¹é‡æ¨é€
        "history_days": 30,     # æ–°å¢ï¼Œä¿ç•™30å¤©
        "bot_token": os.getenv("RSS_LINDA_YOUTUBE"), # Telegram Bot Token
        "processor": {
            "translate": False,                        #ç¿»è¯‘å¼€å…³
            "header_template": "ğŸ“¢ *{source}*\n",  # æ–°å¢æ ‡é¢˜æ¨¡æ¿ â˜…
            "template": "*{subject}*\n[more]({url})",
            "preview": False,                             # ç¦æ­¢é¢„è§ˆ
            "show_count": False                       #è®¡æ•°
        }
    }
]

# ========== æ•°æ®åº“é€‚é…å±‚ ==========

USE_PG = os.getenv("PG_URL") is not None
PG_URL = os.getenv("PG_URL")
if USE_PG:
    import asyncpg

class RSSDatabase:
    def __init__(self, loop=None):
        self.loop = loop or asyncio.get_event_loop()
        self.conn = None
        self.pg_pool = None

    async def open(self):
        if USE_PG:
            self.pg_pool = await asyncpg.create_pool(PG_URL)
        else:
            import sqlite3
            self.conn = sqlite3.connect(DATABASE_FILE, check_same_thread=False)

    async def close(self):
        if USE_PG and self.pg_pool:
            await self.pg_pool.close()
        elif self.conn:
            self.conn.close()

    async def create_tables(self):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                await conn.execute("""
                CREATE TABLE IF NOT EXISTS rss_status (
                    feed_group TEXT,
                    feed_url TEXT,
                    entry_url TEXT,
                    entry_content_hash TEXT,
                    entry_timestamp DOUBLE PRECISION,
                    PRIMARY KEY (feed_group, feed_url, entry_url)
                );
                """)
                await conn.execute("""
                CREATE UNIQUE INDEX IF NOT EXISTS idx_group_content_hash
                ON rss_status(feed_group, entry_content_hash);
                """)
                await conn.execute("""
                CREATE TABLE IF NOT EXISTS timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_run_time DOUBLE PRECISION
                );
                """)
                await conn.execute("""
                CREATE TABLE IF NOT EXISTS cleanup_timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_cleanup_time DOUBLE PRECISION
                );
                """)
                await conn.execute("""
                CREATE TABLE IF NOT EXISTS pending_messages (
                    feed_group TEXT,
                    feed_url TEXT,
                    entry_id TEXT,
                    content_hash TEXT,
                    title TEXT,
                    translated_title TEXT,
                    link TEXT,
                    summary TEXT,
                    entry_timestamp DOUBLE PRECISION,
                    sent INTEGER DEFAULT 0,
                    PRIMARY KEY (feed_group, feed_url, entry_id)
                );
                """)
                await conn.execute("""
                CREATE TABLE IF NOT EXISTS batch_timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_batch_sent_time DOUBLE PRECISION
                );
                """)
        else:
            def _create():
                c = self.conn.cursor()
                c.execute("""
                CREATE TABLE IF NOT EXISTS rss_status (
                    feed_group TEXT,
                    feed_url TEXT,
                    entry_url TEXT,
                    entry_content_hash TEXT,
                    entry_timestamp REAL,
                    PRIMARY KEY (feed_group, feed_url, entry_url)
                )""")
                c.execute("""
                CREATE UNIQUE INDEX IF NOT EXISTS idx_group_content_hash
                ON rss_status(feed_group, entry_content_hash);
                """)
                c.execute("""
                CREATE TABLE IF NOT EXISTS timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_run_time REAL
                )""")
                c.execute("""
                CREATE TABLE IF NOT EXISTS cleanup_timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_cleanup_time REAL
                )""")
                c.execute("""
                CREATE TABLE IF NOT EXISTS pending_messages (
                    feed_group TEXT,
                    feed_url TEXT,
                    entry_id TEXT,
                    content_hash TEXT,
                    title TEXT,
                    translated_title TEXT,
                    link TEXT,
                    summary TEXT,
                    entry_timestamp REAL,
                    sent INTEGER DEFAULT 0,
                    PRIMARY KEY (feed_group, feed_url, entry_id)
                )
                """)
                c.execute("""
                CREATE TABLE IF NOT EXISTS batch_timestamps (
                    feed_group TEXT PRIMARY KEY,
                    last_batch_sent_time REAL
                )
                """)
                self.conn.commit()
            await self.loop.run_in_executor(None, _create)

    # å¾…å‘æ¶ˆæ¯æ“ä½œ
    async def add_pending_message(self, feed_group, feed_url, entry_id, content_hash, title, translated_title, link, summary, timestamp, feed_title):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                await conn.execute("""
                INSERT INTO pending_messages (feed_group, feed_url, entry_id, content_hash, title, translated_title, link, summary, entry_timestamp, sent)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, 0)
                ON CONFLICT DO NOTHING
                """, feed_group, feed_url, entry_id, content_hash, title, translated_title, link, summary, timestamp)
        else:
            def _add():
                c = self.conn.cursor()
                c.execute("""
                    INSERT OR IGNORE INTO pending_messages
                    (feed_group, feed_url, entry_id, content_hash, title, translated_title, link, summary, entry_timestamp, sent, feed_title)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 0, ?)
                """, (feed_group, feed_url, entry_id, content_hash, title, translated_title, link, summary, timestamp, feed_title))
                self.conn.commit()
            await self.loop.run_in_executor(None, _add)

    async def get_pending_messages(self, feed_group):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                rows = await conn.fetch("""
                    SELECT * FROM pending_messages
                    WHERE feed_group=$1 AND sent=0
                    ORDER BY entry_timestamp ASC
                """, feed_group)
                return [dict(row) for row in rows]
        else:
            def _get():
                c = self.conn.cursor()
                c.execute("""
                    SELECT * FROM pending_messages
                    WHERE feed_group=? AND sent=0
                    ORDER BY entry_timestamp ASC
                """, (feed_group,))
                keys = [d[0] for d in c.description]
                return [dict(zip(keys, row)) for row in c.fetchall()]
            return await self.loop.run_in_executor(None, _get)

    async def mark_pending_as_sent(self, feed_group, ids):
        if not ids:
            return
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                await conn.executemany("""
                    UPDATE pending_messages SET sent=1
                    WHERE feed_group=$1 AND entry_id=$2
                """, [(feed_group, eid) for eid in ids])
        else:
            def _mark():
                c = self.conn.cursor()
                c.executemany("""
                    UPDATE pending_messages SET sent=1
                    WHERE feed_group=? AND entry_id=?
                """, [(feed_group, eid) for eid in ids])
                self.conn.commit()
            await self.loop.run_in_executor(None, _mark)

    async def get_last_batch_sent_time(self, feed_group):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                row = await conn.fetchrow("""
                    SELECT last_batch_sent_time FROM batch_timestamps WHERE feed_group=$1
                """, feed_group)
                return row['last_batch_sent_time'] if row else 0
        else:
            def _get():
                c = self.conn.cursor()
                c.execute("""
                    SELECT last_batch_sent_time FROM batch_timestamps WHERE feed_group=?
                """, (feed_group,))
                result = c.fetchone()
                return result[0] if result else 0
            return await self.loop.run_in_executor(None, _get)

    async def save_last_batch_sent_time(self, feed_group, ts):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                await conn.execute("""
                INSERT INTO batch_timestamps (feed_group, last_batch_sent_time)
                VALUES ($1, $2)
                ON CONFLICT (feed_group) DO UPDATE SET last_batch_sent_time=EXCLUDED.last_batch_sent_time
                """, feed_group, ts)
        else:
            def _save():
                c = self.conn.cursor()
                c.execute("""
                    INSERT OR REPLACE INTO batch_timestamps (feed_group, last_batch_sent_time)
                    VALUES (?, ?)
                """, (feed_group, ts))
                self.conn.commit()
            await self.loop.run_in_executor(None, _save)

    # RSSçŠ¶æ€æ“ä½œ
    async def save_status(self, feed_group, feed_url, entry_url, entry_content_hash, timestamp):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                await conn.execute(
                    "INSERT INTO rss_status (feed_group, feed_url, entry_url, entry_content_hash, entry_timestamp) "
                    "VALUES($1, $2, $3, $4, $5) ON CONFLICT DO NOTHING",
                    feed_group, feed_url, entry_url, entry_content_hash, timestamp
                )
        else:
            def _save():
                c = self.conn.cursor()
                c.execute(
                    "INSERT OR IGNORE INTO rss_status VALUES (?, ?, ?, ?, ?)",
                    (feed_group, feed_url, entry_url, entry_content_hash, timestamp)
                )
                self.conn.commit()
            await self.loop.run_in_executor(None, _save)

    async def has_content_hash(self, feed_group, content_hash):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                row = await conn.fetchrow(
                    "SELECT 1 FROM rss_status WHERE feed_group=$1 AND entry_content_hash=$2 LIMIT 1",
                    feed_group, content_hash
                )
                return row is not None
        else:
            def _has():
                c = self.conn.cursor()
                c.execute(
                    "SELECT 1 FROM rss_status WHERE feed_group=? AND entry_content_hash=? LIMIT 1",
                    (feed_group, content_hash)
                )
                return c.fetchone() is not None
            return await self.loop.run_in_executor(None, _has)

    async def load_status(self):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                rows = await conn.fetch("SELECT feed_url, entry_url FROM rss_status")
                status = {}
                for row in rows:
                    feed_url, entry_url = row['feed_url'], row['entry_url']
                    status.setdefault(feed_url, set()).add(entry_url)
                return status
        else:
            def _load():
                c = self.conn.cursor()
                c.execute("SELECT feed_url, entry_url FROM rss_status")
                status = {}
                for feed_url, entry_url in c.fetchall():
                    status.setdefault(feed_url, set()).add(entry_url)
                return status
            return await self.loop.run_in_executor(None, _load)

    async def load_last_run_time(self, feed_group):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                row = await conn.fetchrow("SELECT last_run_time FROM timestamps WHERE feed_group=$1", feed_group)
                return row['last_run_time'] if row else 0
        else:
            def _load():
                c = self.conn.cursor()
                c.execute("SELECT last_run_time FROM timestamps WHERE feed_group = ?", (feed_group,))
                result = c.fetchone()
                return result[0] if result else 0
            return await self.loop.run_in_executor(None, _load)

    async def save_last_run_time(self, feed_group, last_run_time):
        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                await conn.execute("""
                INSERT INTO timestamps (feed_group, last_run_time)
                VALUES ($1, $2)
                ON CONFLICT (feed_group) DO UPDATE SET last_run_time=EXCLUDED.last_run_time
                """, feed_group, last_run_time)
        else:
            def _save():
                c = self.conn.cursor()
                c.execute("""
                    INSERT OR REPLACE INTO timestamps (feed_group, last_run_time)
                    VALUES (?, ?)
                """, (feed_group, last_run_time))
                self.conn.commit()
            await self.loop.run_in_executor(None, _save)

    async def cleanup_history(self, days, feed_group):
        now = time.time()
        cutoff_ts = now - days * 86400

        if USE_PG:
            async with self.pg_pool.acquire() as conn:
                row = await conn.fetchrow(
                    "SELECT last_cleanup_time FROM cleanup_timestamps WHERE feed_group=$1", feed_group
                )
                last_cleanup = row['last_cleanup_time'] if row else 0
                if now - last_cleanup < 86400:
                    return
                await conn.execute(
                    "DELETE FROM rss_status WHERE feed_group=$1 AND entry_timestamp<$2",
                    feed_group, cutoff_ts
                )
                await conn.execute("""
                    INSERT INTO cleanup_timestamps (feed_group, last_cleanup_time)
                    VALUES ($1, $2)
                    ON CONFLICT (feed_group) DO UPDATE SET last_cleanup_time=EXCLUDED.last_cleanup_time
                """, feed_group, now)
        else:
            def _cleanup():
                c = self.conn.cursor()
                c.execute(
                    "SELECT last_cleanup_time FROM cleanup_timestamps WHERE feed_group = ?",
                    (feed_group,)
                )
                result = c.fetchone()
                last_cleanup = result[0] if result else 0
                if now - last_cleanup < 86400:
                    return
                c.execute(
                    "DELETE FROM rss_status WHERE feed_group=? AND entry_timestamp < ?",
                    (feed_group, cutoff_ts)
                )
                c.execute("""
                    INSERT OR REPLACE INTO cleanup_timestamps (feed_group, last_cleanup_time)
                    VALUES (?, ?)
                """, (feed_group, now))
                self.conn.commit()
            await self.loop.run_in_executor(None, _cleanup)
# ========== ä¸šåŠ¡é€»è¾‘ ==========

def remove_html_tags(text):
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'#([^#\s]+)#', r'\1', text)
    text = re.sub(r'#\w+', '', text)
    text = re.sub(r'@[^\s]+', '', text).strip()
    text = re.sub(r'ã€\s*ã€‘', '', text)
    text = re.sub(r'(?<!\S)#(?!\S)', '', text)
    text = re.sub(r'(?<!\S)ï¼š(?!\S)', '', text)
    return text

def get_entry_identifier(entry):
    if hasattr(entry, 'guid') and entry.guid:
        return hashlib.sha256(entry.guid.encode()).hexdigest()
    link = getattr(entry, 'link', '')
    if link:
        try:
            parsed = urlparse(link)
            clean_link = parsed._replace(query=None, fragment=None).geturl().lower()
            return hashlib.sha256(clean_link.encode()).hexdigest()
        except Exception as e:
            logger.warning(f"URLè§£æå¤±è´¥ {link}: {e}")
    title = getattr(entry, 'title', '')
    pub_date = get_entry_timestamp(entry).isoformat() if get_entry_timestamp(entry) else ''
    return hashlib.sha256(f"{title}|||{pub_date}".encode()).hexdigest()

def get_entry_content_hash(entry):
    title = getattr(entry, 'title', '') or ''
    summary = getattr(entry, 'summary', '') or ''
    pub_date = ''
    if hasattr(entry, 'published'):
        pub_date = entry.published
    elif hasattr(entry, 'updated'):
        pub_date = entry.updated
    raw_text = (title.strip() + summary.strip() + pub_date.strip()).encode('utf-8')
    return hashlib.sha256(raw_text).hexdigest()

def signal_handler(signum, frame):
    logger.warning(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œç¨‹åºå³å°†é€€å‡ºã€‚")
    sys.exit(0)
def get_entry_timestamp(entry):
    dt = datetime.now(pytz.UTC)
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        dt = datetime(*entry.published_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'pubDate_parsed') and entry.pubDate_parsed:
        dt = datetime(*entry.pubDate_parsed[:6], tzinfo=pytz.utc)
    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        dt = datetime(*entry.updated_parsed[:6], tzinfo=pytz.utc)
    return dt

@retry(
    stop=stop_after_attempt(1),
    wait=wait_exponential(multiplier=1, min=5, max=30),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def send_single_message(bot, chat_id, text, disable_web_page_preview=False):
    try:
        MAX_MESSAGE_LENGTH = 4096
        text_chunks = []
        current_chunk = []
        current_length = 0
        paragraphs = text.split('\n\n')
        for para in paragraphs:
            para_length = len(para)  # å­—ç¬¦é•¿åº¦
            if current_length + para_length + 2 > MAX_MESSAGE_LENGTH:
                text_chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(para)
            current_length += para_length + 2
        if current_chunk:
            text_chunks.append('\n\n'.join(current_chunk))
        for chunk in text_chunks:
            await bot.send_message(
                chat_id=chat_id,
                text=chunk,
                parse_mode='MarkdownV2',
                disable_web_page_preview=disable_web_page_preview,
                read_timeout=10,
                write_timeout=10
            )
    except BadRequest as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥(Markdowné”™è¯¯): {e} - æ–‡æœ¬ç‰‡æ®µ: {chunk[:200]}...")
    except Exception as e:
        raise

@retry(
    stop=stop_after_attempt(1),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError)),
)
async def fetch_feed(session, feed_url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}
    parsed = urlparse(feed_url)
    is_rsshub = parsed.netloc == "rsshub.app"
    if is_rsshub:
        try_domains = BACKUP_DOMAINS + ["rsshub.app"]
        canonical_url = feed_url.replace(parsed.netloc, "rsshub.app")
    else:
        try_domains = [parsed.netloc]
        canonical_url = feed_url
    for domain in try_domains:
        modified_url = feed_url.replace(parsed.netloc, domain)
        try:
            async with semaphore:
                async with session.get(modified_url, headers=headers, timeout=30) as response:
                    if response.status in (503, 403, 404, 429):
                        continue
                    response.raise_for_status()
                    return parse(await response.read()), canonical_url
        except aiohttp.ClientResponseError as e:
            if e.status in (503, 403, 404, 429):
                continue
        except Exception as e:
        #    logger.error(f"è¯·æ±‚å¤±è´¥: {modified_url}, é”™è¯¯: {e}")
            continue
    logger.error(f"æ‰€æœ‰åŸŸåå°è¯•å¤±è´¥: {feed_url}")
    return None, canonical_url

async def translate_with_credentials(secret_id, secret_key, text):
    loop = asyncio.get_running_loop()
    text_bytes = text.encode('utf-8')
    if len(text_bytes) > 2000:
        safe_bytes = text_bytes[:2000]
        while safe_bytes[-1] & 0xC0 == 0x80:
            safe_bytes = safe_bytes[:-1]
        text = safe_bytes.decode('utf-8', errors='ignore')
        logger.warning(f"æ–‡æœ¬æˆªæ–­è‡³ {len(text)} å­—ç¬¦ ({len(safe_bytes)} å­—èŠ‚)")
    try:
        return await loop.run_in_executor(
            None, 
            lambda: _sync_translate(secret_id, secret_key, text)
        )
    except Exception as e:
        logger.error(f"ç¿»è¯‘æ‰§è¡Œå¤±è´¥: {type(e).__name__} - {str(e)}")
        raise

def _sync_translate(secret_id, secret_key, text):
    try:
        cred = credential.Credential(secret_id, secret_key)
        clientProfile = ClientProfile(httpProfile=HttpProfile(endpoint="tmt.tencentcloudapi.com"))
        client = tmt_client.TmtClient(cred, TENCENT_REGION, clientProfile)
        req = models.TextTranslateRequest()
        req.SourceText = remove_html_tags(text)
        req.Source = "auto"
        req.Target = "zh"
        req.ProjectId = 0
        return client.TextTranslate(req).TargetText
    except TencentCloudSDKException as e:
        error_details = {
            "code": getattr(e, "code", ""),
            "message": getattr(e, "message", str(e)),
            "request_id": getattr(e, "request_id", ""),
            "region": TENCENT_REGION
        }
        logger.error(f"è…¾è®¯äº‘APIé”™è¯¯è¯¦æƒ…: {error_details}")
        raise
    except Exception as e:
        logger.error(f"ç¿»è¯‘è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        raise

@retry(
    stop=stop_after_attempt(2),
    wait=wait_exponential(multiplier=1, min=2, max=10),
)
async def auto_translate_text(text):
    try:
        try:
            return await translate_with_credentials(
                TENCENTCLOUD_SECRET_ID, 
                TENCENTCLOUD_SECRET_KEY,
                text
            )
        except TencentCloudSDKException as e:
            logger.error(f"ä¸»å¯†é’¥ç¿»è¯‘å¤±è´¥: [Code: {e.code}] {e.message}")
            raise
        except Exception as e:
            logger.error(f"ä¸»å¯†é’¥ç¿»è¯‘æœªçŸ¥é”™è¯¯: {type(e).__name__} - {str(e)}")
            raise
    except Exception as first_error:
        if TENCENT_SECRET_ID and TENCENT_SECRET_KEY:
            logger.warning("ä¸»ç¿»è¯‘å¯†é’¥å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨å¯†é’¥...")
            try:
                return await translate_with_credentials(
                    TENCENT_SECRET_ID,
                    TENCENT_SECRET_KEY,
                    text
                )
            except TencentCloudSDKException as e:
                logger.error(f"å¤‡ç”¨å¯†é’¥ç¿»è¯‘å¤±è´¥: [Code: {e.code}] {e.message}")
                raise
            except Exception as e:
                logger.error(f"å¤‡ç”¨å¯†é’¥ç¿»è¯‘æœªçŸ¥é”™è¯¯: {type(e).__name__} - {str(e)}")
                raise
        else:
            logger.error("ä¸»ç¿»è¯‘å¯†é’¥å¤±è´¥ï¼Œä¸”æœªé…ç½®å¤‡ç”¨å¯†é’¥")
            raise first_error
    except Exception as final_error:
        logger.error(f"æ‰€æœ‰ç¿»è¯‘å°è¯•å‡å¤±è´¥: {type(final_error).__name__}")
        cleaned = remove_html_tags(text)
        return escape(cleaned)

async def generate_group_message(feed_data, entries, processor):
    try:
        source_name = feed_data.feed.get('title', "æœªçŸ¥æ¥æº")
        safe_source = escape(source_name)
        header = ""
        if "header_template" in processor:
            header = processor["header_template"].format(source=safe_source) + "\n"
        messages = []
        for entry in entries:
            raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
            if processor["translate"]:
                translated_subject = await auto_translate_text(raw_subject)
            else:
                translated_subject = raw_subject
            safe_subject = escape(translated_subject)
            raw_url = entry.link
            safe_url = escape(raw_url)
            message = processor["template"].format(
                subject=safe_subject,
                source=safe_source,
                url=safe_url
            )
            messages.append(message)
        full_message = header + "\n\n".join(messages)
        if processor.get("show_count", True):
            full_message += f"\n\nâœ… æ–°å¢ {len(messages)} æ¡å†…å®¹"
        return full_message
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ¶ˆæ¯å¤±è´¥: {str(e)}")
        return ""

# ========== æ–°å¢: æ‰¹é‡æ¶ˆæ¯å‘é€ ==========
async def process_batch_send(group, db: RSSDatabase):
    group_key = group["group_key"]
    bot_token = group["bot_token"]
    processor = group["processor"]
    batch_interval = group.get("batch_send_interval")
    if not batch_interval:
        return
    now = datetime.now(pytz.utc).timestamp()
    last_batch_sent = await db.get_last_batch_sent_time(group_key)
    if now - last_batch_sent < batch_interval:
        return
    # è·å–æ‰€æœ‰è¯¥ç»„çš„æœªå‘é€æ¶ˆæ¯
    pending = await db.get_pending_messages(group_key)
    if not pending:
        await db.save_last_batch_sent_time(group_key, now)
        return

    # æŒ‰ feed_url åˆ†ç»„
    from collections import defaultdict
    feed_url_to_msgs = defaultdict(list)
    for row in pending:
        feed_url_to_msgs[row["feed_url"]].append(row)

    bot = Bot(token=bot_token)
    for feed_url, msgs in feed_url_to_msgs.items():
        # feed_title ä¼˜å…ˆç”¨å­˜å‚¨çš„å­—æ®µ
        feed_title = (msgs[0].get("feed_title") or group.get("name") or feed_url)
        class DummyFeed:
            feed = {'title': feed_title}
        class Entry:
            def __init__(self, row):
                self.title = row["translated_title"] or row["title"]
                self.link = row["link"]
        entries = [Entry(row) for row in msgs]
        try:
            feed_message = await generate_group_message(DummyFeed, entries, {**processor, "translate": False})
            if feed_message:
                await send_single_message(
                    bot,
                    TELEGRAM_CHAT_ID[0],
                    feed_message,
                    disable_web_page_preview=not processor.get("preview", True)
                )
                await db.mark_pending_as_sent(group_key, [row["entry_id"] for row in msgs])
        except Exception as e:
            logger.error(f"æ‰¹é‡æ¨é€å¤±è´¥[{group_key}-{feed_url}]: {e}")

    await db.save_last_batch_sent_time(group_key, now)

# ========== ç»„é‡‡é›†ï¼ˆé‡‡é›†ä½†å¯é€‰æ‹©æ˜¯å¦ç«‹å³æ¨é€ï¼‰ ==========
async def process_group(session, group_config, global_status, db: RSSDatabase):
    group_name = group_config["name"]
    group_key = group_config["group_key"]
    processor = group_config["processor"]
    bot_token = group_config["bot_token"]
    batch_send_interval = group_config.get("batch_send_interval", None)
    try:
        last_run = await db.load_last_run_time(group_key)
        now = datetime.now(pytz.utc).timestamp()
        if (now - last_run) < group_config["interval"]:
            return
        bot = Bot(token=bot_token)
        for index, feed_url in enumerate(group_config["urls"]):
            try:
                if index > 0:
                    await asyncio.sleep(1)
                feed_data, canonical_url = await fetch_feed(session, feed_url)
                if not feed_data or not feed_data.entries:
                    continue
                processed_ids = global_status.get(canonical_url, set())
                new_entries = []
                seen_in_batch = set()
                for entry in feed_data.entries:
                    entry_id = get_entry_identifier(entry)
                    content_hash = get_entry_content_hash(entry)
                    if await db.has_content_hash(group_key, content_hash):
                        continue
                    if entry_id in processed_ids or entry_id in seen_in_batch:
                        continue
                    seen_in_batch.add(entry_id)
                    filter_config = processor.get("filter", {})
                    if filter_config.get("enable", False):
                        raw_title = remove_html_tags(entry.title or "")
                        keywords = filter_config.get("keywords", [])
                        match = any(kw.lower() in raw_title.lower() for kw in keywords)
                        if filter_config.get("mode", "allow") == "allow":
                            if not match:
                                continue
                        else:
                            if match:
                                continue
                    new_entries.append((entry, content_hash, entry_id))
                if new_entries:
                    if batch_send_interval:
                        for entry, content_hash, entry_id in new_entries:
                            raw_subject = remove_html_tags(getattr(entry, "title", "") or "")
                            if processor["translate"]:
                                translated_subject = await auto_translate_text(raw_subject)
                            else:
                                translated_subject = raw_subject
                            await db.add_pending_message(
                                group_key, canonical_url, entry_id, content_hash,
                                getattr(entry, "title", ""), translated_subject, getattr(entry, "link", ""), getattr(entry, "summary", ""),
                                get_entry_timestamp(entry).timestamp() if get_entry_timestamp(entry) else time.time(),
                                feed_data.feed.get('title', "") 
                            )
                            await db.save_status(group_key, canonical_url, entry_id, content_hash, time.time())
                            processed_ids.add(entry_id)
                        global_status[canonical_url] = processed_ids
                    else:
                        feed_message = await generate_group_message(feed_data, [e for e,_,_ in new_entries], processor)
                        if feed_message:
                            try:
                                await send_single_message(
                                    bot,
                                    TELEGRAM_CHAT_ID[0],
                                    feed_message,
                                    disable_web_page_preview=not processor.get("preview", True)
                                )
                                for entry, content_hash, entry_id in new_entries:
                                    await db.save_status(group_key, canonical_url, entry_id, content_hash, time.time())
                                    processed_ids.add(entry_id)
                                global_status[canonical_url] = processed_ids
                            except Exception as send_error:
                                logger.error(f"âŒ å‘é€æ¶ˆæ¯å¤±è´¥ [{feed_url}]")
                                raise
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¤±è´¥ [{feed_url}]")
        await db.save_last_run_time(group_key, now)
    except Exception as e:
        logger.critical(f"â€¼ï¸ å¤„ç†ç»„å¤±è´¥ [{group_key}]")

async def main():
    lock_file = None
    db = RSSDatabase()
    try:
        lock_file = open(LOCK_FILE, "w")
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
    except OSError:
        logger.warning("â›” æ— æ³•è·å–æ–‡ä»¶é”ï¼Œå·²æœ‰å®ä¾‹åœ¨è¿è¡Œï¼Œç¨‹åºé€€å‡º")
        return
    except Exception as e:
        logger.critical(f"â€¼ï¸ æ–‡ä»¶é”å¼‚å¸¸: {str(e)}")
        return
    try:
        await db.open()
        await db.create_tables()
    except Exception as e:
        logger.critical(f"â€¼ï¸ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return
    for group in RSS_GROUPS:
        days = group.get("history_days", 30)
        try:
            await db.cleanup_history(days, group["group_key"])
        except Exception as e:
            logger.error(f"æ¸…ç†å†å²è®°å½•å¼‚å¸¸: ç»„={group['group_key']}, é”™è¯¯={e}")
    async with aiohttp.ClientSession() as session:
        try:
            status = await db.load_status()
            tasks = []
            for group in RSS_GROUPS:
                try:
                    tasks.append(process_group(session, group, status, db))
                except Exception as e:
                    logger.error(f"âš ï¸ åˆ›å»ºä»»åŠ¡å¤±è´¥ [{group['name']}]: {str(e)}")
            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                for res in results:
                    if isinstance(res, Exception):
                        logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {res}")
            else:
                logger.warning("â›” æœªåˆ›å»ºä»»ä½•å¤„ç†ä»»åŠ¡")
            batch_tasks = [process_batch_send(group, db) for group in RSS_GROUPS if group.get("batch_send_interval")]
            if batch_tasks:
                await asyncio.gather(*batch_tasks)
        except asyncio.CancelledError:
            logger.warning("â¹ï¸ ä»»åŠ¡è¢«å–æ¶ˆ")
        except Exception as e:
            logger.critical(f"â€¼ï¸ ä¸»å¾ªç¯å¼‚å¸¸: {str(e)}", exc_info=True)
        finally:
            try:
                await session.close()
            except Exception as e:
                logger.error(f"âš ï¸ å…³é—­ä¼šè¯å¤±è´¥: {str(e)}")
    try:
        if lock_file:
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
            lock_file.close()
    except Exception as e:
        logger.error(f"âš ï¸ é‡Šæ”¾æ–‡ä»¶é”å¤±è´¥: {str(e)}")
    await db.close()

if __name__ == "__main__":
    for s in (signal.SIGINT, signal.SIGTERM):
        signal.signal(s, signal_handler)
    try:
        asyncio.run(main())
    except Exception as e:
        logger.critical(f"â€¼ï¸ ä¸»è¿›ç¨‹æœªæ•è·å¼‚å¸¸: {str(e)}", exc_info=True)
        sys.exit(1)